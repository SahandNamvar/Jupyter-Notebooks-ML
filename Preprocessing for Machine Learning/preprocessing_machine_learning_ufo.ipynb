{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e7ef14-39d6-4d90-8182-3564a580ec5e",
   "metadata": {},
   "source": [
    "# Preprocessing UFO Sighting Data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "276edd01-9de6-4bc7-843c-a61100d9c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ufo = pd.read_csv(\"ufo_sightings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e06269-2858-479a-8980-cedba4758112",
   "metadata": {},
   "source": [
    "### Checking column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eabba512-4140-43a7-8e9e-e631d127c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            4935 non-null   object \n",
      " 1   city            4926 non-null   object \n",
      " 2   state           4516 non-null   object \n",
      " 3   country         4255 non-null   object \n",
      " 4   type            4776 non-null   object \n",
      " 5   seconds         4935 non-null   float64\n",
      " 6   length_of_time  4792 non-null   object \n",
      " 7   desc            4932 non-null   object \n",
      " 8   recorded        4935 non-null   object \n",
      " 9   lat             4935 non-null   object \n",
      " 10  long            4935 non-null   float64\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 424.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            4935 non-null   datetime64[ns]\n",
      " 1   city            4926 non-null   object        \n",
      " 2   state           4516 non-null   object        \n",
      " 3   country         4255 non-null   object        \n",
      " 4   type            4776 non-null   object        \n",
      " 5   seconds         4935 non-null   float64       \n",
      " 6   length_of_time  4792 non-null   object        \n",
      " 7   desc            4932 non-null   object        \n",
      " 8   recorded        4935 non-null   object        \n",
      " 9   lat             4935 non-null   object        \n",
      " 10  long            4935 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(8)\n",
      "memory usage: 424.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the DataFrame info\n",
    "print(ufo.info())\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951521c-086d-4ef5-8e84-561639f3326f",
   "metadata": {},
   "source": [
    "### Dropping missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92100bfc-ee37-401e-bb1f-2e1b2eab1d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n",
      "(4283, 11)\n"
     ]
    }
   ],
   "source": [
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[[\"length_of_time\", \"state\", \"type\"]].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing\n",
    "ufo_no_missing = ufo[ufo[\"length_of_time\"].notnull() & ufo[\"state\"].notnull() & ufo[\"type\"].notnull()]\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d4233-3f16-4f39-8e90-ba152fe6019e",
   "metadata": {},
   "source": [
    "## Categorical variables and standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab83be1-9124-4993-9ab0-34388ff3b958",
   "metadata": {},
   "source": [
    "### Extracting numbers from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc8e7e-eb1a-4721-ae3b-8938e31f0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def return_minutes(time_string):\n",
    "    # Search for numbers in time_string\n",
    "    num = re.search(r'\\d+', time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "    return None\n",
    "\n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[['length_of_time', 'minutes']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8d979-a82a-4749-8331-47078e36d8b4",
   "metadata": {},
   "source": [
    "### Identifying features for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7a3b1-aff5-4768-a428-09c56559c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[[\"seconds\", \"minutes\"]].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = np.log(ufo[\"seconds\"])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo[\"seconds_log\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e265c4-eb4c-42b2-b32a-9449c9202ead",
   "metadata": {},
   "source": [
    "## Engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eb318-fd75-4dd2-80e0-192edb804804",
   "metadata": {},
   "source": [
    "### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af748142-e89a-420f-80df-173fe17765d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda val: 1 if val == \"us\" else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo[\"type\"].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo[\"type\"])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ca26b-82ed-4694-9f74-34b69fea0f09",
   "metadata": {},
   "source": [
    "### Features from dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18307c0-4db0-4092-b462-1a17a46550a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo[\"date\"].head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].apply(lambda row: row.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].apply(lambda row: row.year)\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[[\"date\", \"month\", \"year\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff664d-61d3-45d8-8795-9ff2665e6724",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d611d0-6ad8-4d56-8577-b4a83c6753c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the head of the desc field\n",
    "print(ufo[\"desc\"].head())\n",
    "\n",
    "# Create the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Use vec's fit_transform method on the desc field\n",
    "desc_tfidf = vec.fit_transform(ufo[\"desc\"])\n",
    "\n",
    "# Look at the number of columns this creates.\n",
    "print(desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369c3bc-4f04-4ac7-8617-88146107a6b9",
   "metadata": {},
   "source": [
    "## Feature selection and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41a0f9-b4e5-4c13-af17-906eba77cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "print(ufo[[\"seconds\", \"seconds_log\", \"minutes\"]].corr())\n",
    "\n",
    "# Make a list of features to drop   \n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71152590-667d-4d31-a073-24cf9ffb0355",
   "metadata": {},
   "source": [
    "### Modeling the UFO dataset, part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7eb1ba-8bbe-4890-99a6-c20b82d188bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9330e-3913-437f-93fc-300948b09c1e",
   "metadata": {},
   "source": [
    "### Modeling the UFO dataset, part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8f01c-d384-47d6-9d24-922cb2bfed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
