{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b63908-ce20-4522-95c6-2884a474dd78",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c46107-dcde-4c92-99e4-316debc61590",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Initial steps (exploring data types & missing data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb808232-32de-4058-81dc-fbdc18379435",
   "metadata": {},
   "source": [
    "### Exploring missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65b71c61-3243-4d7e-81ca-9b092baecd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opportunity_id          0\n",
       "content_id              0\n",
       "vol_requests            0\n",
       "event_time              0\n",
       "title                   0\n",
       "hits                    0\n",
       "summary                 0\n",
       "is_priority           603\n",
       "category_id            48\n",
       "category_desc          48\n",
       "amsl                  665\n",
       "amsl_unit             665\n",
       "org_title               0\n",
       "org_content_id          0\n",
       "addresses_count         0\n",
       "locality               70\n",
       "region                  0\n",
       "postalcode              6\n",
       "primary_loc           665\n",
       "display_url             0\n",
       "recurrence_type         0\n",
       "hours                   0\n",
       "created_date            0\n",
       "last_modified_date      0\n",
       "start_date_date         0\n",
       "end_date_date           0\n",
       "status                  0\n",
       "Latitude              665\n",
       "Longitude             665\n",
       "Community Board       665\n",
       "Community Council     665\n",
       "Census Tract          665\n",
       "BIN                   665\n",
       "BBL                   665\n",
       "NTA                   665\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "volunteer = pd.read_csv(\"volunteer.csv\")\n",
    "volunteer.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147dcd9-1b6c-411c-bf08-cbb087a0881e",
   "metadata": {},
   "source": [
    "### Dropping missing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e78aa2cd-8f1d-4651-9d36-49e23ba7c4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 33)\n"
     ]
    }
   ],
   "source": [
    "# Drop the Latitude and Longitude columns from volunteer\n",
    "volunteer_cols = volunteer.drop([\"Latitude\", \"Longitude\"], axis=1)\n",
    "\n",
    "# Drop rows with missing category_desc values from volunteer_cols\n",
    "volunteer_subset = volunteer_cols.dropna(subset=[\"category_desc\"])\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bf75e-6bb8-4e71-b5ca-3ede762648f6",
   "metadata": {},
   "source": [
    "### Exploring data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c81a34b-8647-4c9f-b01a-e7a624da5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 665 entries, 0 to 664\n",
      "Data columns (total 35 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   opportunity_id      665 non-null    int64  \n",
      " 1   content_id          665 non-null    int64  \n",
      " 2   vol_requests        665 non-null    int64  \n",
      " 3   event_time          665 non-null    int64  \n",
      " 4   title               665 non-null    object \n",
      " 5   hits                665 non-null    int64  \n",
      " 6   summary             665 non-null    object \n",
      " 7   is_priority         62 non-null     object \n",
      " 8   category_id         617 non-null    float64\n",
      " 9   category_desc       617 non-null    object \n",
      " 10  amsl                0 non-null      float64\n",
      " 11  amsl_unit           0 non-null      float64\n",
      " 12  org_title           665 non-null    object \n",
      " 13  org_content_id      665 non-null    int64  \n",
      " 14  addresses_count     665 non-null    int64  \n",
      " 15  locality            595 non-null    object \n",
      " 16  region              665 non-null    object \n",
      " 17  postalcode          659 non-null    float64\n",
      " 18  primary_loc         0 non-null      float64\n",
      " 19  display_url         665 non-null    object \n",
      " 20  recurrence_type     665 non-null    object \n",
      " 21  hours               665 non-null    int64  \n",
      " 22  created_date        665 non-null    object \n",
      " 23  last_modified_date  665 non-null    object \n",
      " 24  start_date_date     665 non-null    object \n",
      " 25  end_date_date       665 non-null    object \n",
      " 26  status              665 non-null    object \n",
      " 27  Latitude            0 non-null      float64\n",
      " 28  Longitude           0 non-null      float64\n",
      " 29  Community Board     0 non-null      float64\n",
      " 30  Community Council   0 non-null      float64\n",
      " 31  Census Tract        0 non-null      float64\n",
      " 32  BIN                 0 non-null      float64\n",
      " 33  BBL                 0 non-null      float64\n",
      " 34  NTA                 0 non-null      float64\n",
      "dtypes: float64(13), int64(8), object(14)\n",
      "memory usage: 182.0+ KB\n"
     ]
    }
   ],
   "source": [
    "volunteer.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab714b5e-934e-46fa-8c84-d811ba491d28",
   "metadata": {},
   "source": [
    "### Converting a column type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6c5e54e5-a8d5-4e67-89a3-927523279be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opportunity_id          int64\n",
       "content_id              int64\n",
       "vol_requests            int64\n",
       "event_time              int64\n",
       "title                  object\n",
       "hits                    int32\n",
       "summary                object\n",
       "is_priority            object\n",
       "category_id           float64\n",
       "category_desc          object\n",
       "amsl                  float64\n",
       "amsl_unit             float64\n",
       "org_title              object\n",
       "org_content_id          int64\n",
       "addresses_count         int64\n",
       "locality               object\n",
       "region                 object\n",
       "postalcode            float64\n",
       "primary_loc           float64\n",
       "display_url            object\n",
       "recurrence_type        object\n",
       "hours                   int64\n",
       "created_date           object\n",
       "last_modified_date     object\n",
       "start_date_date        object\n",
       "end_date_date          object\n",
       "status                 object\n",
       "Latitude              float64\n",
       "Longitude             float64\n",
       "Community Board       float64\n",
       "Community Council     float64\n",
       "Census Tract          float64\n",
       "BIN                   float64\n",
       "BBL                   float64\n",
       "NTA                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the head of the hits column\n",
    "volunteer[\"hits\"].head()\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer[\"hits\"].astype(int)\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "volunteer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7487f2-aa5a-4a8d-9b8c-4bca9f5bd354",
   "metadata": {},
   "source": [
    "### Training and Test Sets:\n",
    "- trying to predict the 'category_desc' variable using the other features in the dataset.\n",
    "- what is the class distribution (and imbalance) for this label? which descriptions occur less than 50 times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c60a5dbb-73de-4ea4-a0f2-49ed55038afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_desc\n",
       "Strengthening Communities    False\n",
       "Helping Neighbors in Need    False\n",
       "Education                    False\n",
       "Health                       False\n",
       "Environment                   True\n",
       "Emergency Preparedness        True\n",
       "Name: count, dtype: bool"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "volunteer[\"category_desc\"].value_counts() <= 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ca7f7-078d-41a9-b58a-af0f53a43c30",
   "metadata": {},
   "source": [
    "### Stratified sampling:\r",
    "- distribution of class labels in the 'category_desc' column is uneven. To effectively train a model to predict 'category_desc', ensure that the model is trained on a sample of data that is representative of the entire dataset.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0e957-a3d2-44e3-894d-aef4e7ee6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all columns except category_desc\n",
    "X = volunteer.drop(\"category_desc\", axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Print the category_desc counts from y_train\n",
    "print(y_train[\"category_desc\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af325d-11e2-4a8e-8355-007d38fcba91",
   "metadata": {},
   "source": [
    "## Standardization: Scaling & Normalization\n",
    "- Transform continuous data to appear normally distributed\n",
    "- 'wine' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d928f27-680f-47fe-ac83-1b5daf7b85dd",
   "metadata": {},
   "source": [
    "### Modeling without normalizing:\n",
    "- for testing & comparing K-Nearest Neighbors model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c6fc3-5b74-47d2-9020-2a3bd8fdda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"wine_types.csv\")\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Create the k-nearest neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # You can choose the value of K\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data - accuracy\n",
    "print(knn.score(X_test, y_test)) # result: 0.6666666666666666"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a091d-6ecf-484e-b857-bba8664a221a",
   "metadata": {},
   "source": [
    "### Log Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b4139-d7cc-4d91-b617-c5f86dc9ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the variance - 'Proline' columns has the largest variance - candidate for log normalization\n",
    "wine.var()\n",
    "\n",
    "# Print out the variance of the Proline column\n",
    "print(wine[\"Proline\"].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine[\"Proline_log\"] = np.log(wine[\"Proline\"])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine[\"Proline_log\"].var())\n",
    "\n",
    "# Print all\n",
    "print(wine.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116d989-143e-416b-b5fc-3c9d3d9c20f0",
   "metadata": {},
   "source": [
    "### Scaling Data:\n",
    "- we want 'Ash', 'Alcalinity of ash', and 'Magnesium' columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68157fb0-e0cf-4c5b-bd3a-ebd2bfb8979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.describe() # The max of Ash is 3.23, the max of Alcalinity of ash is 30, and the max of Magnesium is 162.\n",
    "\n",
    "# Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales\n",
    "# let's standardize them in a way that allows for use in a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd98ed7-c865-4c6c-98d8-08c7eca1b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Subset the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to wine_subset\n",
    "wine_subset_scaled = scaler.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386a949-481a-4c0c-b4f4-311aa5724d26",
   "metadata": {},
   "source": [
    "### Standardized Data and Modeling: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789c22b-f7ea-4d48-94c5-3c22f9d80e16",
   "metadata": {},
   "source": [
    "#### KNN on non-scaled data:\n",
    "- Before adding standardization, let's look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177698e6-f083-4105-b992-a459e24eff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Create the k-nearest neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=3) \n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data - accuracy\n",
    "print(knn.score(X_test, y_test)) # 0.7111111111111111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124952f-7e6e-4b0c-b324-2bb9d8b344fc",
   "metadata": {},
   "source": [
    "#### KNN on scaled data:\n",
    "- The accuracy score on the unscaled wine dataset was decent, but let's use standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68599c74-4b18-404c-a039-9e76926bf805",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Instantiate a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training and test features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the k-nearest neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=3) \n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test_scaled, y_test)) # 0.9555555555555556 - Improved from 71.11% to 95.56%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc6a25-fede-485b-84df-2f16a040cfd6",
   "metadata": {},
   "source": [
    "## Feature Engineering:\n",
    "- Extract and expand information from existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab8d08-ebb4-40ff-a1ca-b4d5d06d620b",
   "metadata": {},
   "source": [
    "### Encoding categorical variables\n",
    "- 'hiking.json' dataset\n",
    "- There are several columns here that need encoding before they can be modeled, one of which is the 'Accessible' column. Accessible is a binary feature, so it has two values, Y or N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6afbb4b-5dec-4df1-8e4d-043a45f27aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking = pd.read_json(\"hiking.json\")\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[[\"Accessible\", \"Accessible_enc\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43336799-a621-42b1-85a8-eebe6d193a49",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - one-hot\n",
    "- One of the columns in the volunteer dataset, 'category_desc', gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18335274-78bc-44a9-9731-6b8d1da274ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer[\"category_desc\"])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec63be-dc37-475d-8497-a37defb0a8d7",
   "metadata": {},
   "source": [
    "### Aggregating numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8f470-1d2f-4710-bf0a-91556bfdfeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.loc[:, \"run1\": \"run5\"].mean(axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230592b-8437-4a60-aa6d-4c8e45f2c5e7",
   "metadata": {},
   "source": [
    "### Extracting datetime components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afe392-3e92-4618-b855-24a2dacbefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].dt.month\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033446f-b569-4d82-95b3-d5f60e8ccc66",
   "metadata": {},
   "source": [
    "### Extracting string patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89574f2f-2f06-4f1e-be6c-522b9a41afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search(r'\\d+\\.\\d+', length) #'\\d+' search for digits, as many as you can. '\\.' search for period, and so on.\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "\n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5c84e-281b-486d-85e0-b5f9fae4be2d",
   "metadata": {},
   "source": [
    "### Vectorizing text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066bbd8-f827-407f-97c8-b4003c3b5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the title text\n",
    "title_text = volunteer[\"title\"]\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "print(text_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76cda7-f11c-4c7d-b702-5e46cc8a3637",
   "metadata": {},
   "source": [
    "### Text classification using tf/idf vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f6f99-aae6-4fe7-b67c-c82ab6172645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86325c78-9612-4727-bfca-2f483542ff2a",
   "metadata": {},
   "source": [
    "## Feature Selection for Model Creation:\n",
    "- Selecting features to be used for modeling\n",
    "- Doesn't create new features\n",
    "- Improve model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446fc45-5e08-4cdc-b317-f54a5a01e4bb",
   "metadata": {},
   "source": [
    "### Selecting relevant features - Removing redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4aba9d-479f-4e94-9a4e-3a5bc75d4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"locality\", \"region\", \"category_desc\", \"created_date\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of volunteer_subset\n",
    "print(volunteer_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f8c74-ee53-48f6-b1b7-9b8030f6d2c7",
   "metadata": {},
   "source": [
    "### Checking for correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854c22b-8231-43d5-a29c-b10e934f97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the column correlations of the wine dataset\n",
    "print(wine.corr())\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = wine.drop(\"Flavanoids\", axis=1)\n",
    "\n",
    "print(wine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27539349-bd0e-4af9-b8c2-da9a0bf0a3b9",
   "metadata": {},
   "source": [
    "### Exploring text vectors, part 1\n",
    "- Let's expand on the text vector exploration method using the volunteer dataset's title tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921d943-d998-40f0-85ad-0f5518dbf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=8, top_n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86200f-ead4-4f89-9f6c-15f91b02119b",
   "metadata": {},
   "source": [
    "### Exploring text vectors, part 2\n",
    "- Using the return_weights() function you wrote in the previous exercise, we're now going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454cade-6c8c-41c1-a7bd-822767a9aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "        # here we'll call the function from the previous exercise, \n",
    "        # and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, top_n=3)\n",
    "\n",
    "# By converting filtered_words back to a list, \n",
    "# we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51746741-36e3-40b0-8225-c934affd9371",
   "metadata": {},
   "source": [
    "### Training Naive Bayes with feature selection:\n",
    "- re-run the Naive Bayes text classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c373c00-fb55-4a05-a1d8-d85f5c8852fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32dce69-0b37-4502-82da-25b9f60f824e",
   "metadata": {},
   "source": [
    "### Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5636411-bff2-49e2-b723-849ebe6543e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Define the features and labels from the wine dataset\n",
    "X = wine.drop('Type', axis=1)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "pca_X_train = pca.fit_transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ec8cd-4fe1-4b0e-a37a-2a3395a8fdd9",
   "metadata": {},
   "source": [
    "### Training a model with PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917de58-4b98-4c52-94e0-cb36efb06493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit knn to the training data\n",
    "knn.fit(pca_X_train, y_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "print(knn.score(pca_X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
