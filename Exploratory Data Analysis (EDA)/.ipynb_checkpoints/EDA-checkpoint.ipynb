{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2552526f-281d-4217-8107-7fe91fef0474",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) in Python Complete Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c3df0-3b71-46b4-ac18-faec6839d62b",
   "metadata": {},
   "source": [
    "## Initial Exploration\n",
    "- \n",
    "You are researching unemployment rates worldwide and have been given a new dataset to work with. The data has been saved and loaded for you as a pandas DataFrame called unemployment. You've never seen the data before, so your first task is to use a few pandas functions to learn about this new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e6388-b63f-49ee-8784-48699a0721c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of unemployment\n",
    "print(unemployment.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d190a-5c15-457b-bfa5-2c6563d463da",
   "metadata": {},
   "source": [
    "### Counting categorical values\n",
    "- Recall from the previous exercise that the unemployment DataFrame contains 182 rows of country data including country_code, country_name, continent, and unemployment percentages from 2010 through 2021. \r\n",
    "You'd now like to explore the categorical data contained in unemployment to understand the data that it contains related to each continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c651627-8a71-43ca-b851-db71f7f0e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the values associated with each continent in unemployment\n",
    "print(print(unemployment[\"continent\"].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188490e1-a79d-43bb-ba73-37c2f7a45479",
   "metadata": {},
   "source": [
    "### Global unemployment in 2021\r",
    "- \n",
    "It's time to explore some of the numerical data in unemployment! What was typical unemployment in a given year? What was the minimum and maximum unemployment rate, and what did the distribution of the unemployment rates look like across the world? A histogram is a great way to get a sense of the answers to these questions \n",
    "\r\n",
    "Your task in this exercise is to create a histogram showing the distribution of global unemployment rates in 20.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1bd2c3-9d0c-4f27-b1fe-391c57b571a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram of 2021 unemployment; show a full percent in each bin\n",
    "sns.histplot(data=unemployment, x=\"2021\", binwidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834ba91-ad29-439f-8c2e-a3e1bf57ee15",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "### Validating continents\r",
    "- \n",
    "Your colleague has informed you that the data on unemployment from countries in Oceania is not reliable, and you'd like to identify and exclude these countries from your unemployment data. The .isin() function can help with that \n",
    "\r\n",
    "Your task is to use .isin() to identify countries that are not in Oceania. These countries should return True while countries in Oceania should return False. This will set you up to use the results of .isin() to quickly filter out Oceania countries using Boolean indexing.\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab49af-c20d-4ca0-be15-d171e703d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Series describing whether each continent is outside of Oceania\n",
    "not_oceania = ~unemployment[\"continent\"].isin([\"Oceania\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6477f3c-de12-4b43-aca5-3dedeb142155",
   "metadata": {},
   "source": [
    "### Validating range\r",
    "- \n",
    "Now it's time to validate our numerical data. We saw in the previous lesson using .describe() that the largest unemployment rate during 2021 was nearly 34 percent, while the lowest was just above zero \n",
    "\r\n",
    "Your task in this exercise is to get much more detailed information about the range of unemployment data using Seaborn's boxplot, and you'll also visualize the range of unemployment rates in each continent to understand geographical range differenc.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fd960-48a4-4aee-b08d-1202637991df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the minimum and maximum unemployment rates during 2021\n",
    "print(unemployment[\"2021\"].min(), unemployment[\"2021\"].max())\n",
    "\n",
    "# Create a boxplot of 2021 unemployment rates, broken down by continent\n",
    "sns.boxplot(data=unemployment, x=\"2021\", y=\"continent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b8653-e6cb-4023-88a0-add5b1521398",
   "metadata": {},
   "source": [
    "## Data Summarization\n",
    "### Summaries with .groupby() and .agg()\r",
    "- \n",
    "In this exercise, you'll explore the means and standard deviations of the yearly unemployment data. First, you'll find means and standard deviations regardless of the continent to observe worldwide unemployment trends. Then, you'll check unemployment trends broken down by continent.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2dc546-d48c-4437-b3b7-db590cee7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean and standard deviation of rates by year\n",
    "print(unemployment.agg([\"mean\", \"std\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db2c94-77bc-448a-8291-9bec1a5b5249",
   "metadata": {},
   "source": [
    "### Named aggregations\r",
    "- \n",
    "You've seen how .groupby() and .agg() can be combined to show summaries across categories. Sometimes, it's helpful to name new columns when aggregating so that it's clear in the code output what aggregations are being applied and where \n",
    "\r\n",
    "Your task is to create a DataFrame called continent_summary which shows a row for each continent. The DataFrame columns will contain the mean unemployment rate for each continent in 2021 as well as the standard deviation of the 2021 employment rate. And of course, you'll rename the columns so that their contents are cle!\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf703b-25b2-4a7d-829c-331e1828d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "continent_summary = unemployment.groupby(\"continent\").agg(\n",
    "    # Create the mean_rate_2021 column\n",
    "    mean_rate_2021 = (\"2021\", \"mean\"),\n",
    "    # Create the std_rate_2021 column\n",
    "    std_rate_2021 = (\"2021\", \"std\"),\n",
    ")\n",
    "print(continent_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4482b5-fc3c-47eb-9296-6eecf2d0bfae",
   "metadata": {},
   "source": [
    "### Visualizing categorical summaries.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7608e45-daf8-4532-9545-d2dfef7d0f20",
   "metadata": {},
   "source": [
    "- As you've learned in this chapter, Seaborn has many great visualizations for exploration, including a bar plot for displaying an aggregated average value by category of data.\n",
    "- In Seaborn, bar plots include a vertical bar indicating the 95% confidence interval for the categorical mean. Since confidence intervals are calculated using both the number of values and the variability of those values, they give a helpful indication of how much data can be relied upon.\n",
    "- Your task is to create a bar plot to visualize the means and confidence intervals of unemployment rates across the different continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba37629-769e-4ec8-9b83-78ab3a676b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of continents and their average unemployment\n",
    "sns.barplot(data=unemployment, x=\"continent\", y=\"2021\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb25f43-5d72-40f1-8cf5-68f9b33a07b6",
   "metadata": {},
   "source": [
    "## Data Cleaning and Imputations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fe1c9-2569-49e0-9711-99e022ddc2c0",
   "metadata": {},
   "source": [
    "### Addressing missing data:\n",
    "- It is important to deal with missing data before starting your analysis. One approach is to drop missing values if they account for a small proportion, typically five percent, of your data.\n",
    "- Working with a dataset on plane ticket prices, stored as a pandas DataFrame called planes, you'll need to count the number of missing values across all columns, calculate five percent of all values, use this threshold to remove observations, and check how many missing values remain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126f155-f802-4046-acaf-12174afa0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in each column\n",
    "print(planes.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c09d3-bf11-4c1f-b71c-2a7874003aa0",
   "metadata": {},
   "source": [
    "### Strategies for remaining missing data:\n",
    "- The five percent rule has worked nicely for your planes dataset, eliminating missing values from nine out of 11 columns!\n",
    "- Now, you need to decide what to do with the \"Additional_Info\" and \"Price\" columns, which are missing 300 and 368 values respectively.\n",
    "- You'll first take a look at what \"Additional_Info\" contains, then visualize the price of plane tickets by different airlines.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fd555-2e4e-41fa-ba2f-1ff23de99f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of the Additional_Info column\n",
    "print(planes[\"Additional_Info\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426576c-461c-439d-b46d-4c48b1290051",
   "metadata": {},
   "source": [
    "### Imputing missing plane prices\r",
    "- Now there's just one column with missing values left\n",
    "- You've removed the \"Additional_Info\" column from planes—the last step is to impute the missing data in the \"Price\" column of the datas\n",
    "- As a reminder, you generated this boxplot, which suggested that imputing the median price based on the \"Airline\" is a solid approach!\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe62b94-9863-4490-8539-dec97bd91adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median plane ticket prices by Airline\n",
    "airline_prices = planes.groupby(\"Airline\")[\"Price\"].median()\n",
    "\n",
    "print(airline_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2232c0-b8f4-4bde-8601-1514399fcd98",
   "metadata": {},
   "source": [
    "## Converting and Analyzing Categorical Data\n",
    "### Finding the number of unique values\n",
    "- You would like to practice some of the categorical data manipulation and analysis skills that you've just seen. To help identify which data could be reformatted to extract value, you are going to find out which non-numeric columns in the planes dataset have a large number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b4581-0205-4694-97a7-29de7f5dca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for object columns\n",
    "non_numeric = planes.select_dtypes(\"object\")\n",
    "\n",
    "# Loop through columns\n",
    "for col in non_numeric.columns:\n",
    "  \n",
    "  # Print the number of unique values\n",
    "  print(f\"Number of unique values in {col} column: \", non_numeric[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b41b7-ccc5-496f-a811-481fe4dded11",
   "metadata": {},
   "source": [
    "### Flight duration categories\n",
    "- As you saw, there are 362 unique values in the \"Duration\" column of planes. Looks like this won't be simple to convert to numbers. However, you could categorize flights by duration and examine the frequency of different flight lengths! Looks like this won't be simple to convert to numbers. However, you could categorize flights by duration and examine the frequency of different flight lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf47cb-ea6f-411e-b539-272e37f83926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of categories\n",
    "flight_categories = [\"Short-haul\", \"Medium\", \"Long-haul\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c230c4-4ea7-4b22-a41f-fcb3c705f73f",
   "metadata": {},
   "source": [
    "### Adding duration categories\n",
    "- Now that you've set up the categories and values you want to capture, it's time to build a new column to analyze the frequency of flights by duration! The variables flight_categories, short_flights, medium_flights, and long_flights that you previously created are available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a3391-6639-46c1-955f-9266606c993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conditions for values in flight_categories to be created\n",
    "conditions = [\n",
    "    (planes[\"Duration\"].str.contains(short_flights)),\n",
    "    (planes[\"Duration\"].str.contains(medium_flights)),\n",
    "    (planes[\"Duration\"].str.contains(long_flights))\n",
    "]\n",
    "\n",
    "# Apply the conditions list to the flight_categories\n",
    "planes[\"Duration_Category\"] = np.select(conditions, \n",
    "                                        flight_categories,\n",
    "                                        default=\"Extreme duration\")\n",
    "\n",
    "# Plot the counts of each category\n",
    "sns.countplot(data=planes, x=\"Duration_Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a6107-e7b7-4033-9604-ad151b7708d8",
   "metadata": {},
   "source": [
    "## Working with numerica data\n",
    "### Flight duration\n",
    "- You would like to analyze the duration of flights, but unfortunately, the \"Duration\" column in the planes DataFrame currently contains string values. You'll need to clean the column and convert it to the correct data type for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4261c20-1c50-40b0-b8c8-9e407ead9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the column\n",
    "print(planes[\"Duration\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc604e-72ea-4600-b432-405ed099be95",
   "metadata": {},
   "source": [
    "### Adding descriptive statistics\n",
    "- Now \"Duration\" and \"Price\" both contain numeric values in the planes DataFrame, you would like to calculate summary statistics for them that are conditional on values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649d180-b351-4246-a977-efdcdbfb05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price standard deviation by Airline\n",
    "planes[\"airline_price_st_dev\"] = planes.groupby(\"Airline\")[\"Price\"].transform(lambda x: x.std())\n",
    "\n",
    "print(planes[[\"Airline\", \"airline_price_st_dev\"]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c5731-a18b-4d04-a0b0-4967afdc7579",
   "metadata": {},
   "source": [
    "## Handling outliers\n",
    "### Identifying outliers\n",
    "- You've proven that you recognize what to do when presented with outliers, but can you identify them using visualizations? Try to figure out if there are outliers in the \"Price\" or \"Duration\" columns of the planes DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e9fdb-c63a-4c0b-82a1-b321c765e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of flight prices\n",
    "sns.histplot(data=planes, x=\"Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d2da9-0a3f-4e87-bc73-a8b58c874fe4",
   "metadata": {},
   "source": [
    "### Removing outliers\n",
    "- While removing outliers isn't always the way to go, for your analysis, you've decided that you will only include flights where the \"Price\" is not an outlier. Therefore, you need to find the upper threshold and then use it to remove values above this from the planes DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089e15f-7284-4db0-84c0-64a9d958db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 75th and 25th percentiles\n",
    "price_seventy_fifth = planes[\"Price\"].quantile(0.75)\n",
    "price_twenty_fifth = planes[\"Price\"].quantile(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd36d8-b4fe-4c00-b6e6-53934e675626",
   "metadata": {},
   "source": [
    "## Relationships in Data: Patterns over time\n",
    "### Importing DateTime data\n",
    "- You'll now work with the entire divorce dataset! The data describes Mexican marriages dissolved between 2000 and 2015. It contains marriage and divorce dates, education level, birthday, income for each partner, and marriage duration, as well as the number of children the couple had at the time of divorce. It looks like there is a lot of date information in this data that is not yet a DateTime data type! Your task is to fix that so that you can explore patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557e2d3-75ba-4299-a3f8-9f8bea16f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import divorce.csv, parsing the appropriate columns as dates in the import\n",
    "divorce = pd.read_csv(\"divorce.csv\", parse_dates=[\"divorce_date\", \"dob_man\", \"dob_woman\", \"marriage_date\"])\n",
    "print(divorce.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66bc2c-d477-483d-b3e5-507572fffcd6",
   "metadata": {},
   "source": [
    "### Visualizing relationships over time\n",
    "- Now that your date data is saved as DateTime data, you can explore patterns over time! Does the year that a couple got married have a relationship with the number of children that the couple has at the time of divorce? Your task is to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82ea88-0cd1-41b0-9081-bb05fb1b9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the marriage_year column\n",
    "divorce[\"marriage_year\"] = divorce[\"marriage_date\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16f065-f359-4af4-aa46-9b8a0517415c",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "### Visualizing variable relationships\n",
    "- In the last exercise, you may have noticed that a longer marriage_duration is correlated with having more children, represented by the num_kids column. The correlation coefficient between the marriage_duration and num_kids variables is 0.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775c327-68a1-4a76-a743-7b5fbb07d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatterplot\n",
    "sns.scatterplot(data=divorce, x=\"marriage_duration\", y=\"num_kids\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369ca77-93cf-40db-9de4-1b45f0fb9ba1",
   "metadata": {},
   "source": [
    "### Visualizing multiple variable relationships\n",
    "- Seaborn's .pairplot() is excellent for understanding the relationships between several or all variables in a dataset by aggregating pairwise scatter plots in one visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea3179-b69c-4435-b076-f1e3fcfe3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot for income_woman and marriage_duration\n",
    "sns.pairplot(data=divorce, vars=[\"income_woman\", \"marriage_duration\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b03a3-3efb-423c-b98e-13d4d8074826",
   "metadata": {},
   "source": [
    "## Factor relationships and distributions\n",
    "### Categorial data in scatter plots\n",
    "- We explored how men's education and age at marriage related to other variables in our dataset, the divorce DataFrame. Now, you'll take a look at how women's education and age at marriage relate to other variables! Your task is to create a scatter plot of each woman's age and income, layering in the categorical variable of education level for additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497273f8-8219-42a4-9232-7b3e3de4eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "sns.scatterplot(data=divorce, x=\"woman_age_marriage\", y=\"income_woman\", hue=\"education_woman\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cec2c-6791-4e0e-ab84-97a3e7c12cf3",
   "metadata": {},
   "source": [
    "### Exploring with KDE plots\n",
    "- Kernel Density Estimate (KDE) plots are a great alternative to histograms when you want to show multiple distributions in the same visual.\n",
    "- Suppose you are interested in the relationship between marriage duration and the number of kids that a couple has. Since values in the num_kids column range only from one to five, you can plot the KDE for each value on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a180a7a-1094-4241-b837-41055a821d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KDE plot\n",
    "sns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10808918-cc2e-4fab-b3b8-cd8444280a7e",
   "metadata": {},
   "source": [
    "## Considerations for categorical data\n",
    "### Checking for class imbalance\n",
    "- The 2022 Kaggle Survey captures information about data scientists' backgrounds, preferred technologies, and techniques. It is seen as an accurate view of what is happening in data science based on the volume and profile of responders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193963bd-d47b-4a52-be28-9d4fe1dc336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the relative frequency of Job_Category\n",
    "print(salaries[\"Job_Category\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dab346-9d17-49c3-9edc-d2584e86fbbb",
   "metadata": {},
   "source": [
    "### Cross-tabulation\n",
    "- Cross-tabulation can help identify how observations occur in combination.\n",
    "- Using the salaries dataset, which has been imported as a pandas DataFrame, you'll perform cross-tabulation on multiple variables, including the use of aggregation, to see the relationship between \"Company_Size\" and other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4090e-2cac-4aaa-8e9e-bcaada30c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulate Company_Size and Experience\n",
    "print(pd.crosstab(salaries[\"Company_Size\"], salaries[\"Experience\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa469da-e468-475f-bb1b-ae4c9f7e70b8",
   "metadata": {},
   "source": [
    "### Generating new features\n",
    "- Extracting features for correlation In this exercise, you'll work with a version of the salaries dataset containing a new column called \"date_of_response\". The dataset has been read in as a pandas DataFrame, with \"date_of_response\" as a datetime data type. Your task is to extract datetime attributes from this column and then create a heat map to visualize the correlation coefficients between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144ab4e-3bcc-4390-a5ab-f28c4a9f83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the month of the response\n",
    "salaries[\"month\"] = salaries[\"date_of_response\"].dt.month\n",
    "\n",
    "# Extract the weekday of the response\n",
    "salaries[\"weekday\"] = salaries[\"date_of_response\"].dt.weekday\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(salaries.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5af191-79fa-4fb3-bc4f-e75c19258dc4",
   "metadata": {},
   "source": [
    "### Calculating salary percentiles\n",
    "- We've seen that the conversion of numeric data into categories sometimes makes it easier to identify patterns. Your task is to convert the \"Salary_USD\" column into categories based on its percentiles. First, you need to find the percentiles and store them as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0b4e1-53d1-42ff-a503-2f368a6c24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 25th percentile\n",
    "twenty_fifth = salaries[\"Salary_USD\"].quantile(0.25)\n",
    "\n",
    "# Save the median\n",
    "salaries_median = salaries[\"Salary_USD\"].median()\n",
    "\n",
    "# Gather the 75th percentile\n",
    "seventy_fifth = salaries[\"Salary_USD\"].quantile(0.75)\n",
    "print(twenty_fifth, salaries_median, seventy_fifth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb50a4e-f746-4b16-a60b-7ee96fbc57a7",
   "metadata": {},
   "source": [
    "### Categorizing salaries \n",
    "- Now it's time to make a new category! You'll use the variables twenty_fifth, salaries_median, and seventy_fifth, that you created in the previous exercise, to split salaries into different labels. The result will be a new column called \"salary_level\", which you'll incorporate into a visualization to analyze survey respondents' salary and at companies of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99f271-75de-4170-a477-f280d45fb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create salary labels\n",
    "salary_labels = [\"entry\", \"mid\", \"senior\", \"exec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84f278-14c8-40fb-a37f-fb0209c31f92",
   "metadata": {},
   "source": [
    "### Comparing salaries\n",
    "- Exploratory data analysis is a crucial step in generating hypotheses! You've had an idea you'd like to explore—do data professionals get paid more in the USA than they do in Great Britain? You'll need to subset the data by \"Employee_Location\" and produce a plot displaying the average salary between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7d1a5-81b8-4673-9b91-e3249c3a56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for employees in the US or GB\n",
    "usa_and_gb = salaries[salaries[\"Employee_Location\"].isin([\"US\", \"GB\"])]\n",
    "\n",
    "# Create a barplot of salaries by location\n",
    "sns.barplot(data=usa_and_gb, x=\"Employee_Location\", y=\"Salary_USD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f59585-84d1-4f37-8252-32d63593ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of salary versus company size, factoring in employment status\n",
    "sns.barplot(data=salaries, x=\"Company_Size\", y=\"Salary_USD\", hue=\"Employment_Status\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
